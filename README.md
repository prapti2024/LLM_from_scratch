ğŸ§  LLM from Scratch
This project is a minimal and educational implementation of a Generative Pre-trained Transformer (GPT) model, inspired by the paper â€œAttention is All You Needâ€ and projects like Karpathy's minGPT.

The goal is to build a GPT-style language model entirely from scratch, without relying on high-level libraries like HuggingFace. This serves as a hands-on exercise to understand the inner workings of LLMs â€” including tokenization, transformer blocks, attention mechanisms, and training dynamics.

ğŸš€ Features
Character-level tokenizer

Custom embedding layers

Multi-head self-attention

Layer normalization and residual connections

GPT-style decoder-only transformer architecture

Training loop using cross-entropy loss

Sample generation from trained model

ğŸ“ Repository Structure
graphql
Copy
Edit
LLM_from_scratch/
â”‚
â”œâ”€â”€ gpt_from_scratch.ipynb      # Main notebook: build and train GPT step by step
â”œâ”€â”€ README.md                   # Project overview and documentation
ğŸ› ï¸ Requirements
Python 3.8+

PyTorch

NumPy

tqdm (for progress bar)

Install dependencies:

bash
Copy
Edit
pip install torch numpy tqdm
ğŸ§ª How to Run
Clone the repo:

bash
Copy
Edit
git clone https://github.com/prapti2024/LLM_from_scratch.git
cd LLM_from_scratch
Open the Jupyter notebook:

bash
Copy
Edit
jupyter notebook gpt_from_scratch.ipynb
Run cells step-by-step to build the model, train it on a small dataset (like Shakespeare), and generate text samples.

ğŸ“Š Training Example
The model is trained on a small text corpus using character-level prediction. Here's an example of generated text after training:

css
Copy
Edit
Whathasth be thas areson all,
Yoe milde sape spank bores.
(Yes, itâ€™s gibberishâ€”but it demonstrates learned patterns of structure and punctuation!)

ğŸ“š Inspiration
Karpathyâ€™s nanoGPT

Jay Alammarâ€™s Transformer visualizations

Andrej Karpathyâ€™s GPT lectures and blog posts

âœ… TODO
Add positional encodings (sinusoidal/trainable)

Switch to BPE or WordPiece tokenizer

Train on larger datasets

Save and reload model checkpoints

ğŸ§‘â€ğŸ’» Author
Prapti Dahal
GitHub

ğŸ“„ License
This project is open-source and available under the MIT License.
